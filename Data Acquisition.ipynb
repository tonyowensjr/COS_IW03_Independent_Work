{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc511a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import key packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "pd.set_option('mode.chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3213eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dictionaries that will map each team acronym onto the same format\n",
    "class MissingDict(dict):\n",
    "    __missing__ = lambda self, key: key\n",
    "acronyms = {\n",
    "'Arizona' : 'ARI', 'L.A. Dodgers' : 'LAD', 'N.Y. Mets' : 'NYM', 'N.Y. Yankees' : 'NYY', 'Tampa Bay' :  'TBR', \n",
    "'Oakland' : 'OAK', 'Baltimore' : 'BAL', 'St. Louis' : 'STL', 'Kansas City' : 'KCR', \n",
    "'Milwaukee' : 'MIL', 'Toronto' : 'TOR', 'L.A. Angels' : 'LAA', 'Boston' : 'BOS', 'Seattle' : 'SEA', \n",
    "'Pittsburgh' : 'PIT', 'Miami' : 'MIA', 'Cleveland' : 'CLE', 'Texas' : 'TEX', 'Atlanta' : 'ATL', 'Chi. Cubs' : 'CHC', \n",
    "'Chi. White Sox' : 'CHW', 'Detroit' : 'DET', 'Minnesota' : 'MIN', 'Cincinnati' : 'CIN', 'Philadelphia' : 'PHI', \n",
    "'Washington' : 'WSN', 'San Francisco' : 'SFG', 'San Diego' : 'SDP', 'Colorado' : 'COL', 'Houston' : 'HOU'\n",
    "}\n",
    "maps = MissingDict(**acronyms)\n",
    "\n",
    "class MissingDict(dict):\n",
    "    __missing__ = lambda self, key: key\n",
    "conversions = {\n",
    "'Arizona Diamondbacks' : 'ARI', 'Los Angeles Dodgers' : 'LAD', 'New York Mets' : 'NYM', 'New York Yankees' : 'NYY', 'Tampa Bay Rays' :  'TBR', \n",
    "'Oakland Athletics' : 'OAK', 'Baltimore Orioles' : 'BAL', 'St. Louis Cardinals' : 'STL', 'Kansas City Royals' : 'KCR', \n",
    "'Milwaukee Brewers' : 'MIL', 'Toronto Blue Jays' : 'TOR', 'Los Angeles Angels' : 'LAA', 'Boston Red Sox' : 'BOS', 'Seattle Mariners' : 'SEA', \n",
    "'Pittsburgh Pirates' : 'PIT', 'Miami Marlins' : 'MIA', 'Cleveland Indians' : 'CLE', 'Cleveland Guardians' : 'CLE', 'Texas Rangers' : 'TEX', 'Atlanta Braves' : 'ATL', 'Chicago Cubs' : 'CHC', \n",
    "'Chicago White Sox' : 'CHW', 'Detroit Tigers' : 'DET', 'Minnesota Twins' : 'MIN', 'Cincinnati Reds' : 'CIN', 'Philadelphia Phillies' : 'PHI', \n",
    "'Washington Nationals' : 'WSN', 'San Francisco Giants' : 'SFG', 'San Diego Padres' : 'SDP', 'Colorado Rockies' : 'COL', 'Houston Astros' : 'HOU',\n",
    "'Florida Marlins' : 'MIA', 'Montreal Expos' : 'WAS', 'Anaheim Angels' : 'LAA', 'Tampa Bay Devil Rays' : 'TB'\n",
    "}\n",
    "mapping = MissingDict(**conversions)\n",
    "\n",
    "class MissingDict(dict):\n",
    "    __missing__ = lambda self, key: key\n",
    "winner_cons = {\n",
    "'ARI' : 'ARI', 'LAD' : 'LAD', 'NYM' : 'NYM', 'NYY' : 'NYY', 'TB' :  'TBR', \n",
    "'OAK' : 'OAK', 'BAL' : 'BAL', 'STL' : 'STL', 'KC' : 'KCR', \n",
    "'MIL' : 'MIL', 'TOR' : 'TOR', 'LAA' : 'LAA', 'BOS' : 'BOS', 'SEA' : 'SEA', \n",
    "'PIT' : 'PIT', 'MIA' : 'MIA', 'CLE' : 'CLE', 'CLE' : 'CLE', 'TEX' : 'TEX', 'ATL' : 'ATL', 'CHC' : 'CHC', \n",
    "'CHW' : 'CHW', 'DET' : 'DET', 'MIN' : 'MIN', 'CIN' : 'CIN', 'PHI' : 'PHI', \n",
    "'WAS' : 'WSN', 'SF' : 'SFG', 'SD' : 'SDP', 'COL' : 'COL', 'HOU' : 'HOU',\n",
    "'MIA' : 'MIA', 'LAA' : 'LAA', 'TB' : 'TB'\n",
    "}\n",
    "mapping = MissingDict(**winner_cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01336454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the parameters used for the pitcher specific data\n",
    "useful = ['Home Away', 'Home Home', 'Home Winner', 'Home Loser','Home Name','Home W', 'Home L', 'Home SV',\n",
    "       'Home G', 'Home GS', 'Home IP', 'Home K/9', 'Home BB/9', 'Home HR/9',\n",
    "       'Home BABIP', 'Home LOB%', 'Home GB%', 'Home HR/FB', 'Home vFA (pi)',\n",
    "       'Home ERA', 'Home xERA', 'Home FIP', 'Home xFIP', 'Home WAR', 'Away Name','Away W', 'Away L', 'Away SV',\n",
    "       'Away G', 'Away GS', 'Away IP', 'Away K/9', 'Away BB/9', 'Away HR/9',\n",
    "       'Away BABIP', 'Away LOB%', 'Away GB%', 'Away HR/FB', 'Away vFA (pi)',\n",
    "       'Away ERA', 'Away xERA', 'Away FIP', 'Away xFIP', 'Away WAR','Home Run Line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3313fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end day used for each season\n",
    "start_mon,start_day,end_mon,end_day = 3,31,9,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d345c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years included in the data\n",
    "years = [x for x in range(2013,2023) if x != 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff9c68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n",
      "2013-04-09\n",
      "2013-04-19\n",
      "2013-04-29\n",
      "2013-05-09\n",
      "2013-05-19\n",
      "2013-05-29\n",
      "2013-06-08\n",
      "2013-06-18\n",
      "2013-06-28\n",
      "2013-07-08\n",
      "2013-07-28\n",
      "2013-08-07\n",
      "2013-08-17\n",
      "2013-08-27\n",
      "2013-09-06\n",
      "2013-09-16\n",
      "2013-09-26\n",
      "2014\n",
      "2014-04-06\n",
      "2014-04-16\n",
      "2014-04-26\n",
      "2014-05-06\n",
      "2014-05-16\n",
      "2014-05-26\n",
      "2014-06-05\n",
      "2014-06-15\n",
      "2014-06-25\n",
      "2014-07-05\n",
      "2014-07-15\n",
      "2014-07-25\n",
      "2014-08-04\n",
      "2014-08-14\n",
      "2014-08-24\n",
      "2014-09-03\n",
      "2014-09-13\n",
      "2014-09-23\n",
      "2015\n",
      "2015-04-13\n",
      "2015-04-23\n",
      "2015-05-03\n",
      "2015-05-13\n",
      "2015-05-23\n",
      "2015-06-02\n",
      "2015-06-12\n",
      "2015-06-22\n",
      "2015-07-02\n",
      "2015-07-12\n",
      "2015-07-22\n",
      "2015-08-01\n",
      "2015-08-11\n",
      "2015-08-21\n",
      "2015-08-31\n",
      "2015-09-10\n",
      "2015-09-20\n",
      "2016\n",
      "2016-04-10\n",
      "2016-04-20\n",
      "2016-04-30\n",
      "2016-05-10\n",
      "2016-05-20\n",
      "2016-05-30\n",
      "2016-06-09\n",
      "2016-06-29\n",
      "2016-07-09\n",
      "2016-07-19\n",
      "2016-07-29\n",
      "2016-08-08\n",
      "2016-08-18\n",
      "2016-08-28\n",
      "2016-09-07\n",
      "2016-09-17\n",
      "2016-09-27\n",
      "2017\n",
      "2017-04-07\n",
      "2017-04-17\n",
      "2017-04-27\n",
      "2017-05-07\n",
      "2017-05-17\n",
      "2017-05-27\n",
      "2017-06-06\n",
      "2017-06-16\n",
      "2017-06-26\n",
      "2017-07-06\n",
      "2017-07-16\n",
      "2017-07-26\n",
      "2017-08-05\n",
      "2017-08-15\n",
      "2017-08-25\n",
      "2017-09-04\n",
      "2017-09-14\n",
      "2017-09-24\n",
      "2018\n",
      "2018-04-04\n",
      "2018-04-14\n",
      "2018-04-24\n",
      "2018-05-04\n",
      "2018-05-14\n",
      "2018-05-24\n",
      "2018-06-03\n",
      "2018-06-13\n",
      "2018-06-23\n",
      "2018-07-03\n",
      "2018-07-13\n",
      "2018-07-23\n",
      "2018-08-02\n",
      "2018-08-12\n",
      "2018-08-22\n",
      "2018-09-01\n",
      "2018-09-11\n",
      "2018-09-21\n",
      "2019\n",
      "2019-04-01\n",
      "2019-04-11\n",
      "2019-04-21\n",
      "2019-05-01\n",
      "2019-05-11\n",
      "2019-05-21\n",
      "2019-05-31\n",
      "2019-06-10\n",
      "2019-06-20\n",
      "2019-06-30\n",
      "2019-07-20\n",
      "2019-07-30\n",
      "2019-08-09\n",
      "2019-08-19\n",
      "2019-08-29\n",
      "2019-09-08\n",
      "2019-09-18\n",
      "2019-09-28\n",
      "2021\n",
      "2021-04-08\n",
      "2021-04-18\n",
      "2021-04-28\n",
      "2021-05-08\n",
      "2021-05-18\n",
      "2021-05-28\n",
      "2021-06-07\n",
      "2021-06-17\n",
      "2021-06-27\n",
      "2021-07-07\n",
      "2021-07-17\n",
      "2021-07-27\n",
      "2021-08-06\n",
      "2021-08-16\n",
      "2021-08-26\n",
      "2021-09-05\n",
      "2021-09-15\n",
      "2021-09-25\n",
      "2022\n",
      "2022-04-15\n",
      "2022-04-25\n",
      "2022-05-05\n",
      "2022-05-15\n",
      "2022-05-25\n",
      "2022-06-04\n",
      "2022-06-14\n",
      "2022-06-24\n",
      "2022-07-04\n",
      "2022-07-14\n",
      "2022-07-24\n",
      "2022-08-03\n",
      "2022-08-13\n",
      "2022-08-23\n",
      "2022-09-02\n",
      "2022-09-12\n",
      "2022-09-22\n"
     ]
    }
   ],
   "source": [
    "all_stuff = []\n",
    "tt = 0\n",
    "for year in years:\n",
    "    print(year)\n",
    "    start_date = f'{year}-03-31'\n",
    "    all_matches = []\n",
    "    bat_reg =[]\n",
    "    bat_adv = []\n",
    "    pit_reg = []\n",
    "    pit_adv = []\n",
    "    sp_v = []\n",
    "    # iterate through all dates from March 30th to September 30th inclusive\n",
    "    for date1 in np.arange(datetime(year,start_mon,start_day), datetime(year,end_mon,end_day), timedelta(days=1)).astype(datetime):\n",
    "        tt += 1\n",
    "        time.sleep(.2)\n",
    "        date = date1.strftime(\"%Y-%m-%d\")\n",
    "        try:\n",
    "\n",
    "            # acquire the schedule for the given day from cbs\n",
    "            cbs_url = f\"https://www.cbssports.com/mlb/schedule/{date.replace('-','')}/\"\n",
    "            matches = pd.read_html(requests.get(cbs_url).content)[0]\n",
    "            matches['Away'] = matches['Away'].map(acronyms)\n",
    "            matches['Home'] = matches['Home'].map(acronyms)\n",
    "            matches['Winner'] = np.empty(len(matches.index))\n",
    "            matches['Loser'] = np.empty(len(matches.index))\n",
    "            matches.insert(1,\"Date\",[date] * len(matches.index))\n",
    "            for j in matches.index:\n",
    "                matches.loc[j,'Winner'] = str(matches[matches.index == j]['Result']).split('    ')[1].split(' ')[0]\n",
    "                matches.loc[j,'Loser'] = str(matches[matches.index == j]['Result']).split('    ')[1].split(' ')[3]\n",
    "            all_matches.append(matches)\n",
    "            time.sleep(.2)\n",
    "            # get the regular hitting stats from fangraphs\n",
    "            fg_url = f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=0&season={year}a&month=1000&season1={year}&ind=0&team=0%2Cts&rost=0&age=0&filter=&players=0&startdate={start_date}&enddate={date}'\n",
    "            c = pd.read_html(requests.get(fg_url).content)[-2].iloc[:30,:]\n",
    "            c.columns = c.columns.droplevel()\n",
    "            c.insert(1,'Date',[date] * 30)\n",
    "            bat_reg.append(c)\n",
    "            time.sleep(.2)\n",
    "            # get the advanced hitting stats from fangraphs \n",
    "            fg_url = f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=1&season={year}&month=1000&season1={year}&ind=0&team=0,ts&rost=0&age=0&filter=&players=0&startdate={start_date}&enddate={date}'\n",
    "            c = pd.read_html(requests.get(fg_url).content)[-2].iloc[:30,:]\n",
    "            c.columns = c.columns.droplevel()\n",
    "            c.insert(1,'Date',[date] * 30)\n",
    "            bat_adv.append(c)\n",
    "            time.sleep(.2)\n",
    "            # get the regular pitching stats from fangraphs\n",
    "            fg_url = f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=0&season={year}&month=1000&season1={year}&ind=0&team=0,ts&rost=0&age=0&filter=&players=0&startdate={start_date}&enddate={date}'\n",
    "            c = pd.read_html(requests.get(fg_url).content)[-2].iloc[:30,:]\n",
    "            c.columns = c.columns.droplevel()\n",
    "            c.insert(1,'Date',[date] * 30)\n",
    "            pit_reg.append(c)\n",
    "            time.sleep(.2)\n",
    "            # get the advanced pitching stats from fangraphs\n",
    "            fg_url = f'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=1&season={year}&month=1000&season1={year}&ind=0&team=0%2Cts&rost=0&age=0&filter=&players=0&startdate={start_date}&enddate={date}'\n",
    "            c = pd.read_html(requests.get(fg_url).content)[-2].iloc[:30,:]\n",
    "            c.columns = c.columns.droplevel()\n",
    "            c.insert(1,'Date',[date] * 30)\n",
    "            pit_adv.append(c)\n",
    "            if not tt % 10:\n",
    "                print(date)\n",
    "        except ValueError or KeyError:\n",
    "            continue\n",
    "    #combine the stats for a given year\n",
    "    bat_reg1 = pd.concat(bat_reg)\n",
    "    bat_adv1 = pd.concat(bat_adv)\n",
    "    pit_reg1 = pd.concat(pit_reg)\n",
    "    pit_adv1 = pd.concat(pit_adv)\n",
    "\n",
    "    pit_reg1.drop(columns = '#',inplace=True)\n",
    "    pit_adv1.drop(columns = '#',inplace= True)\n",
    "    p_cols = [x for x in pit_reg1.columns if x in pit_adv1.columns]\n",
    "\n",
    "    pit_stats = pd.merge(pit_reg1,pit_adv1,on = p_cols)\n",
    "    # merge the pitching stats\n",
    "    bat_reg1.drop(columns = '#',inplace=True)\n",
    "    bat_adv1.drop(columns = '#',inplace= True)\n",
    "\n",
    "    b_cols = [x for x in bat_reg1.columns if x in bat_adv1.columns]\n",
    "    # merge the hitting stats\n",
    "    hit_stats = pd.merge(bat_reg1,bat_adv1,on = b_cols)\n",
    "    # filter out games without a result (postponed games),the result is denoted by the score in the format a-b\n",
    "    # where a is the winning team's score and b is the losing team's score\n",
    "    df = pd.concat(all_matches)\n",
    "    df = df[df.Result.str.count('-') > 0]\n",
    "\n",
    "    # create dataframes with the hitting and pitching stats\n",
    "    hit_df = df.merge(hit_stats.add_prefix('Home_Hit_'),left_on = ['Date','Home'],right_on = ['Home_Hit_Date','Home_Hit_Team']).merge(hit_stats.add_prefix('Away_Hit_'),left_on = ['Date','Away'],right_on = ['Away_Hit_Date','Away_Hit_Team'])\n",
    "    pit_df =  df.merge(pit_stats.add_prefix('Home_Pit_'),left_on = ['Date','Home'],right_on = ['Home_Pit_Date','Home_Pit_Team']).merge(pit_stats.add_prefix('Away_Pit_'),left_on = ['Date','Away'],right_on = ['Away_Pit_Date','Away_Pit_Team'])\n",
    "\n",
    "    # merge them together and drop all duplicates\n",
    "    final_df =pd.merge(hit_df,pit_df,on = [x for x in hit_df.columns if x in pit_df.columns],how = 'inner').drop_duplicates(subset = [x for x in hit_df.columns if x in pit_df.columns])\n",
    "    final_df = final_df.drop_duplicates(subset = ['Home','Away','Date'])\n",
    "    final_df = final_df[final_df.Result.str.count('-') > 0]\n",
    "    all_stuff.append(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e69cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the unwanted columns\n",
    "useless = ['Result',\n",
    " 'Pitcher (W)',\n",
    " 'Pitcher (L)',\n",
    " 'Pitcher (S)',\n",
    " 'Home_Hit_Date',\n",
    " 'Home_Hit_Team',\n",
    " 'Home_Hit_TG',\n",
    " 'Away_Hit_Date',\n",
    " 'Away_Hit_Team',\n",
    " 'Away_Hit_TG',\n",
    " 'Home_Pit_Date',\n",
    " 'Home_Pit_Team',\n",
    " 'Home_Pit_TG',\n",
    " 'Away_Pit_Date',\n",
    " 'Away_Pit_Team',\n",
    " 'Away_Pit_TG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aac9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out unnecessary columns\n",
    "cols = [x for x in list(pd.concat(all_stuff).columns) if x not in useless]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc314bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = pd.concat(all_stuff)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c5aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for differences between the cbs and fangraphs acronyms\n",
    "col_convs = {'WSN':'WAS','KCR':'KC','SDP':'SD','SFG':'SF','TBR':'TB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcd9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df.Home = fin_df.Home.replace(col_convs)\n",
    "fin_df.Away = fin_df.Away.replace(col_convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbeecd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df['Target'] = fin_df.Home == fin_df.Winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40c86f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21020, 158)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38520b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "downloads_path = str(Path.home() / \"Downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fea649e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data set as a pickle object\n",
    "fin_df.to_pickle(downloads_path + '/data_w_correct_cols.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df5de8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data set as a csv file\n",
    "fin_df.to_csv(downloads_path + '/data_w_correct_cols.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
